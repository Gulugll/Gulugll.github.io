<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>粒子群算法（PSO）</title>
      <link href="/2024/01/27/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95%EF%BC%88PSO%EF%BC%89/"/>
      <url>/2024/01/27/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95%EF%BC%88PSO%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h1><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>​    粒子群算法具有收敛速度快、参数少、算法简单容易实现的优点，但是会有局部最优解的情况发生。</p><h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p>​    粒子群算法主要模拟鸟类觅食的行为，所以我们设想我们的“小鸟”在森林中随机搜索，每一只”小鸟“都向着自己判断的方向搜索，并记录搜索过程中食物最多的位置，同时进行共享，这样“小鸟”们就知道哪个为止的食物最多。在整个搜索过程中每只“小鸟”都会根据鸟群当前的信息来调整自己的搜索策略。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><h2 id="6个重要参数"><a href="#6个重要参数" class="headerlink" title="6个重要参数"></a>6个重要参数</h2><ol><li>粒子位置$X_{id}$</li><li>粒子速度$V_{id}$</li><li>第i个粒子搜索到的最优位置$P_{i,pbest}$</li><li>群体搜索的最优为止$P_{d,gbest}$</li><li>第i个例子搜索到的最优位置的优化目标函数值$f_p$</li><li>群体搜索的最优位置的优化目标函数值$f_g$</li></ol><h2 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h2><img src="/2024/01/27/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95%EF%BC%88PSO%EF%BC%89/v2-1542915a08c301af314c13eddc9cbb88_1440w.webp" class="" title="v2-1542915a08c301af314c13eddc9cbb88_1440w"><h1 id="参数详解"><a href="#参数详解" class="headerlink" title="参数详解"></a>参数详解</h1><h2 id="粒子群规模：N"><a href="#粒子群规模：N" class="headerlink" title="粒子群规模：N"></a>粒子群规模：N</h2><p>​    没啥好说的，越多越好，但是随着数量的增加，“性价比”会很低。</p><h2 id="粒子维度：D"><a href="#粒子维度：D" class="headerlink" title="粒子维度：D"></a>粒子维度：D</h2><p>​    根据优化目标函数的自变量个数进行设置。</p><h2 id="迭代次数：K"><a href="#迭代次数：K" class="headerlink" title="迭代次数：K"></a>迭代次数：K</h2><p>​    字面意思，特点和粒子群规模差不多。</p><blockquote><p>（经典取值：60、70、100）</p></blockquote><h2 id="惯性权重：-omega"><a href="#惯性权重：-omega" class="headerlink" title="惯性权重：$\omega$"></a>惯性权重：$\omega$</h2><p>​    动态调整惯性权重以平衡收敛的全局性以及收敛速度。</p><h3 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h3><p>​    表示上一代粒子的速度对当代粒子速度的影响。$\omega$越大，探索新区域的能力越强，全局搜索能力越大（受群体影响小），但是局部寻优能力弱。<strong>所以，我们希望$\omega$不是一个定值，而是前期大后期小，在前期提高全局搜索能力，在后期提高局部搜索能力时期提升收敛速度。</strong></p><blockquote><p>特别的，当$\omega$=1时，退化为基本粒子群算法，当$\omega$=0时，粒子失去对本身经验的思考。</p><p>推荐取值：0.9、1.2、1.5、1.8</p></blockquote><h3 id="改善-omega"><a href="#改善-omega" class="headerlink" title="改善$\omega$"></a>改善$\omega$</h3><p>​    从上文可以得知<strong>我们希望$\omega$不是一个定值</strong>，所以我们采用一种常用的自适应调整策略——线性变化策略：在每次迭代过程中线性地减少$\omega$:</p><script type="math/tex; mode=display">\omega =\omega _{max}-(\omega_{max}-\omega_{min})\frac{iter}{iter_{max}}</script><h2 id="学习因子：-c-1-c-2"><a href="#学习因子：-c-1-c-2" class="headerlink" title="学习因子：$c_1,c_2$"></a>学习因子：$c_1,c_2$</h2><p>​    也称作加速系数或者加速因子。</p><ul><li>$c<em>1$表示粒子下一步动作中自身经验部分的占比，指将粒子推向个体最优位置$P</em>{id,pbest}$的加速权重；</li><li>$c<em>2$表示粒子下一步动作来源于其他粒子经验部分的占比，指将粒子推向群体最优位置$P^k</em>{d,gbest}$的加速权重；</li><li>$c_1$=0时称为无私型粒子群算法，容易陷入局部最优解；</li><li>$c_2$=0时称为自我认识粒子群算法，没有信息共享，会导致算法收敛慢甚至无法收敛；</li><li>$c_1\times c_2\neq 0$时称之为完全粒子群算法，一般最优。</li></ul><blockquote><p>经典取值：$c_1=c_2=2$、 $c_1=1.6，c_2=1.8$、$c_1=1.6，c_2=2$</p></blockquote><h1 id="速度更新公式"><a href="#速度更新公式" class="headerlink" title="速度更新公式"></a>速度更新公式</h1><p>（都是向量）</p><script type="math/tex; mode=display">v^{k+1}_{id}=\omega v^{k}_{id}+c_1r_1(p^{k}_{id,pbest}-x^{k}_{id})+c_2r_2(p^k_{d,gbest}-x^{k}_{id})</script><h2 id="1-速度公式解释"><a href="#1-速度公式解释" class="headerlink" title="1.速度公式解释"></a>1.速度公式解释</h2><ol><li>第一项：惯性部分</li><li>第二项：认知部分</li><li>第三项：社会部份</li></ol><h2 id="2-参数定义"><a href="#2-参数定义" class="headerlink" title="2.参数定义"></a>2.参数定义</h2><p>$N$——粒子群规模；$i$——粒子序号；</p><p>$D$——粒子维度；$d$——粒子维度序号；</p><p>$k$——迭代次数；$\omega$——惯性系数；$c_1$——个体学习因子；$c_2$——群体学习因子；</p><p>$r_1,r_2$——区间[0,1]内的随机数，增加搜索随机性；</p><p>$v^k_{id}$——粒子i在第k次迭代中在第d维的速度向量；</p><p>$x^k_{id}$——粒子i在第k次迭代中在第d维的位置向量；</p><p>$p^k_{id,pbest}$——粒子i在第k次迭代中第d维的历史最优位置，即在第k次迭代后，第i个粒子得到的最优解；</p><p>$p^k_{id,gbest}$——群体在第k次迭代中第d维的历史最优为止，即在第k次迭代后，整个例子群体中的最优解。</p><h1 id="位置更新公式"><a href="#位置更新公式" class="headerlink" title="位置更新公式"></a>位置更新公式</h1><script type="math/tex; mode=display">x^{k+1}_{id}=x^{k}_{id}+v^{k+1}_{id}</script><h1 id="matlab代码（二维）"><a href="#matlab代码（二维）" class="headerlink" title="matlab代码（二维）"></a>matlab代码（二维）</h1><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">clear </span><br><span class="line">clc</span><br><span class="line"></span><br><span class="line"><span class="comment">%绘制原图        图1目标函数的三维网格图</span></span><br><span class="line">x1=<span class="number">-15</span>:<span class="number">0.1</span>:<span class="number">15</span></span><br><span class="line">x2=<span class="number">-15</span>:<span class="number">0.1</span>:<span class="number">15</span>;</span><br><span class="line">[x1,x2]=<span class="built_in">meshgrid</span>(x1,x2);</span><br><span class="line"></span><br><span class="line">y=(x1.^<span class="number">2</span>)/<span class="number">3</span>+(x2.^<span class="number">2</span>)/<span class="number">4</span>;</span><br><span class="line">mesh(x1,x2,y);    </span><br><span class="line"><span class="built_in">hold</span> on;</span><br><span class="line"></span><br><span class="line"><span class="comment">%%预设参数</span></span><br><span class="line">n=<span class="number">300</span>; <span class="comment">%粒子群的规模</span></span><br><span class="line">d=<span class="number">2</span>; <span class="comment">%变量个数</span></span><br><span class="line">c1=<span class="number">2</span>;</span><br><span class="line">c2=<span class="number">2</span>;</span><br><span class="line">w_max=<span class="number">1</span>;</span><br><span class="line">w_min=<span class="number">0.1</span>;</span><br><span class="line">w=<span class="number">0</span>;</span><br><span class="line">K=<span class="number">300</span>; <span class="comment">%迭代次数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%分布粒子的取值范围及速度的取值范围</span></span><br><span class="line">x=<span class="number">-10</span>+<span class="number">20</span>*<span class="built_in">rand</span>(n,d);  <span class="comment">%x在[-10,10]中取值</span></span><br><span class="line">v=<span class="number">-5</span>+<span class="number">10</span>*<span class="built_in">rand</span>(n,d); <span class="comment">%v在[-5,5]中取值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%计算适应度</span></span><br><span class="line">fit=<span class="built_in">zeros</span>(n,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span>=<span class="number">1</span>:n</span><br><span class="line">    fit(<span class="built_in">j</span>)=A11_01(x(<span class="built_in">j</span>,:));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">pbest=x;<span class="comment">%个人最优点即为初始点</span></span><br><span class="line">ind=<span class="built_in">find</span>(<span class="built_in">min</span>(fit)==fit);<span class="comment">% 找最小值</span></span><br><span class="line">gbest=x(ind,:);<span class="comment">%群体最优点</span></span><br><span class="line">h=<span class="built_in">scatter3</span>(x(:,<span class="number">1</span>),x(:,<span class="number">2</span>),fit,<span class="string">&#x27;o&#x27;</span>);  <span class="comment">%图2 粒子的初始分布图</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%%更新速度与位置</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:K<span class="comment">%迭代</span></span><br><span class="line">    <span class="keyword">for</span> m=<span class="number">1</span>:n<span class="comment">%按维度更新</span></span><br><span class="line">       w=w_max-(w_max-w_min)*(<span class="built_in">i</span>/K);</span><br><span class="line">       v(m,:)=w*v(m,:) + c1*<span class="built_in">rand</span>*(pbest(m,:)-x(m,:)) + c2*<span class="built_in">rand</span>*(gbest-x(m,:));<span class="comment">%rand是[0,1]随机数</span></span><br><span class="line">       <span class="comment">%限制速度</span></span><br><span class="line">       v(m,<span class="built_in">find</span>(v(m,:)&lt;<span class="number">-5</span>))=<span class="number">-5</span>;<span class="comment">%这里发现速度小于-5时取-5</span></span><br><span class="line">       v(m,<span class="built_in">find</span>(v(m,:)&gt;<span class="number">5</span>))=<span class="number">5</span>;<span class="comment">%这里发现速度大于5时取5</span></span><br><span class="line">       </span><br><span class="line">       x(m,:)=x(m,:)+v(m,:);<span class="comment">%位置更新</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">%限制位置</span></span><br><span class="line">       x(m,<span class="built_in">find</span>(x(m,:)&lt;<span class="number">-10</span>))=<span class="number">-10</span>;<span class="comment">%这里发现位置小于-10时取-10</span></span><br><span class="line">       x(m,<span class="built_in">find</span>(x(m,:)&gt;<span class="number">10</span>))=<span class="number">10</span>;<span class="comment">%这里发现位置大于10时取10</span></span><br><span class="line">       </span><br><span class="line">       <span class="comment">%重新计算适应度</span></span><br><span class="line">       fit(m)=A11_01(x(m,:));</span><br><span class="line">       <span class="keyword">if</span> x(m,:)&lt;A11_01(pbest(m,:))</span><br><span class="line">           pbest(m,:)=x(m,:);</span><br><span class="line">       <span class="keyword">end</span></span><br><span class="line">       <span class="keyword">if</span> A11_01(pbest(m,:))&lt;A11_01(gbest)<span class="comment">%小</span></span><br><span class="line">           gbest=pbest(m,:);</span><br><span class="line">       <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    fitnessbest(<span class="built_in">i</span>)=A11_01(gbest);<span class="comment">%最优点  </span></span><br><span class="line">    pause(<span class="number">0.01</span>);    <span class="comment">%为了直观，每更新一次，暂停0.01秒</span></span><br><span class="line">    h.XData=x(:,<span class="number">1</span>);</span><br><span class="line">    h.YData=x(:,<span class="number">2</span>);</span><br><span class="line">    h.ZData=fit;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">hold</span> off;</span><br><span class="line"><span class="built_in">plot</span>(fitnessbest);</span><br><span class="line">xlabel(<span class="string">&#x27;迭代次数&#x27;</span>);</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span>=<span class="title">A11_01</span><span class="params">(x)</span></span></span><br><span class="line">    y=x(<span class="number">1</span>)^<span class="number">2</span>/<span class="number">3</span>+x(<span class="number">2</span>)^<span class="number">2</span>/<span class="number">4</span>;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h1 id="我们需要重点考虑的问题"><a href="#我们需要重点考虑的问题" class="headerlink" title="我们需要重点考虑的问题"></a>我们需要重点考虑的问题</h1><h2 id="1-适应值"><a href="#1-适应值" class="headerlink" title="1.适应值"></a>1.适应值</h2><p>就是优化目标函数的值。</p><h2 id="2-位置限制"><a href="#2-位置限制" class="headerlink" title="2.位置限制"></a>2.位置限制</h2><p>自变量范围，就是定义域。</p><h2 id="3-速度限制"><a href="#3-速度限制" class="headerlink" title="3.速度限制"></a>3.速度限制</h2><p>跑太快会错过最优解。</p><blockquote><p>$v_{max}$一般设置为例子变化范围的10%~20%.</p></blockquote><h2 id="4-优化停止准测"><a href="#4-优化停止准测" class="headerlink" title="4.优化停止准测"></a>4.优化停止准测</h2><ol><li>达到最大迭代步数</li><li>达到满意解</li></ol><h2 id="5-初始化"><a href="#5-初始化" class="headerlink" title="5.初始化"></a>5.初始化</h2><p>​    粒子群算法优化的结果受很多因素的影响，其中受粒子初始值的影响比较大，而且较难调控。如果粒子初始值是随机初始化的，在不改变任何参数的情况下，多次优化的结果不一定都收敛到一个全局或局部最优解，也可能会得到一个无效解。所以粒子初始化是一个十分重要的步骤，它关系到整个优化过程中优化收敛的速度与方向。如果粒子的初始化范围选择得好的话可以大大缩短优化的收敛时间，也不易于陷入局部最优解。我们需要根据具体的问题进行分析，如果根据我们的经验判断出最优解一定在某个范围内，则就在这个范围内初始化粒子。<strong>如果无法确定，则以粒子的取值边界作为初始化范围。</strong></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://zhuanlan.zhihu.com/p/346355572">粒子群优化算法(Particle Swarm Optimization, PSO)的详细解读 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/qq_41053605/article/details/88924287">Matlab粒子群算法（PSO）优化程序——经典实例_matlab粒子群算法(pso)优化程序-CSDN博客</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 智能算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jupyter疑难杂症</title>
      <link href="/2023/12/06/jupyter%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/"/>
      <url>/2023/12/06/jupyter%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87/</url>
      
        <content type="html"><![CDATA[<h1 id="添加anaconda环境"><a href="#添加anaconda环境" class="headerlink" title="添加anaconda环境"></a>添加anaconda环境</h1><p>首先安装ipykernel：conda install ipykernel</p><p>在虚拟环境下创建kernel文件：conda install -n 环境名称 ipykernel</p><p>激活conda环境： source activate 环境名称</p><p>将环境写入notebook的kernel中</p><p>python -m ipykernel install —user —name 环境名称 —display-name “Python (环境名称)”</p><p>删除kernel环境：</p><p>jupyter kernelspec remove 环境名称</p>]]></content>
      
      
      
        <tags>
            
            <tag> 工具使用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大规模无线传感器网络压缩数据采集</title>
      <link href="/2023/12/03/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%BA%BF%E4%BC%A0%E6%84%9F%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"/>
      <url>/2023/12/03/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%BA%BF%E4%BC%A0%E6%84%9F%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>​    压缩感知主要应用于图像处理等领域，但是如果我们利用其特性，它在物理网领域也是可以得到较好的运用的，这篇博客主要是讲一种Compressive Data Gathering（CDG）方法在物联网数据收集中的应用。</p><h1 id="采用压缩感知的优势"><a href="#采用压缩感知的优势" class="headerlink" title="采用压缩感知的优势"></a>采用压缩感知的优势</h1><p>​    在物联网传输信息的过程中，我们单独抽出一条链路传输信息的过程来看.</p><img src="/2023/12/03/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%BA%BF%E4%BC%A0%E6%84%9F%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/cdg1.png" class="" title="cdg1"><p>​    我们可以看到，在数据向Sink节点传输的过程中，单对传输这个过程来说，他的复杂度是$O(n^2)$的，同时，在传输的过程中，整个能量的消耗是不均匀的，越接近Sink节点的子节点能量消耗越大，这对于整个无线传感器网络的效率与维护是不利的，所以我们采用压缩感知算法来优化这一过程。</p><h1 id="如何采用压缩感知算法"><a href="#如何采用压缩感知算法" class="headerlink" title="如何采用压缩感知算法"></a>如何采用压缩感知算法</h1><p>如下图所示：</p><img src="/2023/12/03/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%BA%BF%E4%BC%A0%E6%84%9F%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/cdg2.png" class="" title="cdg2"><p>​    图a是一个常见的无线传感器网络，可以看到图中的Sink节点有四个子树（用虚线划分），我们取出其中一个子树作为样例来解释如何进行压缩，我们假设对节点$S_i$收集到的数据标记为$x_i$,那么根据压缩感知的理论我们就可以得到这样一个压缩的结果，如果对压缩感知没有概念的同学可以去看看我的上一篇文章：<a href="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" title="压缩感知基本概念">压缩感知基本概念</a></p><script type="math/tex; mode=display">y_i=\sum_{j=1}^N\phi_{ij}x_j</script><p>​    对应到矩阵运算中就是</p><script type="math/tex; mode=display">\begin{Bmatrix}y_1\\y_2\\\vdots\\y_M\end{Bmatrix}=\begin{Bmatrix}\phi_{11} &\phi_{12} &... &\phi_{1N}\\\phi_{21} &\phi_{22} &... &\phi_{2N}\\\vdots &&\vdots\\\phi_{M1} &\phi_{M2} &... &\phi_{MN}\end{Bmatrix}\begin{Bmatrix}x_1\\x_2\\\vdots\\x_M\end{Bmatrix}</script><p>​    于是乎，我们的数据收集就由原来的$O(N^2)$复杂度变为现在的$O(NM)$复杂度，如下图</p><img src="/2023/12/03/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%BA%BF%E4%BC%A0%E6%84%9F%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/cdg3.png" class="" title="cdg3"><p>​    在上图中我们看到，每个数据点的传输从原来的1，2，3，4…，N个数据到现在的每个都是M个数据，不光降低了复杂度，同时也使得在整个传输过程中能量的均匀分布。</p><h1 id="数据还原"><a href="#数据还原" class="headerlink" title="数据还原"></a>数据还原</h1><p>​    OK，下面就是这篇文章的重中之重了，如何进行数据的还原，毕竟你也不想数据还原以后误差很大吧。</p><h2 id="数据的稀疏化"><a href="#数据的稀疏化" class="headerlink" title="数据的稀疏化"></a>数据的稀疏化</h2><p>​    这个在我上一篇文章中讲过，这里简单复述下。</p><p>​    由于压缩感知只有在对于稀疏信号的情况下才可以完整地还原信号，所以我们的想法就是找到一种变换方式使得在以下公式中我们得到的s是一个稀疏矩阵</p><script type="math/tex; mode=display">x_i=\sum_{j=1}^N\psi_{ij} s_j</script><p>​    而在这篇文章中，作者采用的是离散余弦转换（DCT），但是这种转换只有部分情况才会产生离散情况，具体怎么产生，后面会讲。</p><p>​    当然，还有一个问题就是我们应该怎么确定M的取值呢，这里作者直接给出了一个公式</p><script type="math/tex; mode=display">M\geq c \cdot \mu^2(\Phi ,\Psi)\cdot K\cdot log N</script><p>​    其中</p><script type="math/tex; mode=display">\mu(\Phi, \Psi)=\sqrt{N} \cdot\mathop{\max}_{1\leq i,j\leq N} | \langle \phi_i,\psi_j \rangle |</script><p>​    通过计算我们可以大体得出M在3K~4K左右通常就可以满足我们的还原需求。</p><h2 id="具体的还原算法"><a href="#具体的还原算法" class="headerlink" title="具体的还原算法"></a>具体的还原算法</h2><p>我看了下原文的方法和常见的还原算法差不多，就直接看我整理的这篇吧：<a href="/2023/12/03/%E5%B8%B8%E8%A7%81%E8%BF%98%E5%8E%9F%E7%AE%97%E6%B3%95/" title="常见还原算法">常见还原算法</a></p><h2 id="异常值的处理"><a href="#异常值的处理" class="headerlink" title="异常值的处理"></a>异常值的处理</h2><p>​    原文的意思是这样的，如下图</p><img src="/2023/12/03/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%97%A0%E7%BA%BF%E4%BC%A0%E6%84%9F%E5%99%A8%E7%BD%91%E7%BB%9C%E5%8E%8B%E7%BC%A9%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/dct4.png" class="" title="dct4"><p>​    左图是正常情况下的信号以及其在DCT变换后的图像，右图是存在两个异常读数（两个圈出来的值）的情况下的DCT变换后的图像，可以看到右图存在异常读入的情况下DCT变换后的值明显是非稀疏，所以作者采用了一些方法来规避掉这种情况。</p><p>​    首先我们将我们的数据分解为两个部分</p><script type="math/tex; mode=display">x=x_0+x_s</script><p>​    其中$x_0$为在某一变换域中的稀疏读数，$x_s$为异常读书的偏差值，由于异常读数是零星的，所以$x_s$在时域上是一个稀疏信号</p><p>​    那么根据之前我们得到的性质，我们可以得到：</p><script type="math/tex; mode=display">x=\Psi s_0+I s_s</script><p>​    其中$I$为单位矩阵。这时我们得到的两个向量$s_0,s_s$都是稀疏的，也就是说我们将原信号分解成了两个不同域中的稀疏信号，这时候我们定义一个<strong>完备基</strong>：$\Psi ’ =[\Psi I]$，那么我们的原始信号就可以表示为(其实就是简单的线性代数)：</p><script type="math/tex; mode=display">x=\Psi' s,s=[s_0^T s_s^T]^T</script><p>​    </p><p>​    然后我们就把这个值丢到我们的还原算法里就好啦！</p><p>​    <strong>其实我觉得作者的这种做法是有点问题的，但是我没有仔细研究过DCT变换或者小波变换，也没法说他这里有啥问题，反正我在仿真的时候在一些情况下没有异常值的时候DCT变换也是会出问题的，这里挖个坑吧，研究下这些变换后看看是怎么个事，说不定就是一篇文章了（</strong></p><h1 id="网络容量分析"><a href="#网络容量分析" class="headerlink" title="网络容量分析"></a>网络容量分析</h1><p>​    </p>]]></content>
      
      
      
        <tags>
            
            <tag> 压缩感知 </tag>
            
            <tag> 文献阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见还原算法</title>
      <link href="/2023/12/03/%E5%B8%B8%E8%A7%81%E8%BF%98%E5%8E%9F%E7%AE%97%E6%B3%95/"/>
      <url>/2023/12/03/%E5%B8%B8%E8%A7%81%E8%BF%98%E5%8E%9F%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>​    如果对压缩感知没有概念的同学可以去看看我的上一篇文章：<a href="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" title="压缩感知基本概念">压缩感知基本概念</a></p><h1 id="基追踪算法"><a href="#基追踪算法" class="headerlink" title="基追踪算法"></a>基追踪算法</h1><p>​    我们考虑下面的问题：</p><script type="math/tex; mode=display">Ax=b</script><p>​    其中$x\in R^n,b\in R^m,A\in R^{m\times n},m&lt;&lt;n$,实际上，这个问题是存在无穷多解的，但是对于压缩感知而言，我们需要的是<strong>稀疏解</strong>，所以我们需要将问题转化成</p><script type="math/tex; mode=display">min||x||_0 \quad s.t.\quad Ax=b</script><p>​    但是我们不难发现，这是一个NP-hard问题，但是在这类问题中，我们可以将$||x||_1与||x||_0$等价看待，由于$||x||_1$是连续的，对于我们来说就可以更好地进行求解。<strong>但是</strong>，还有一个问题，$l_2$范数比$l_1$范数更好求解，那么我们可以通过求解$min||x||_2$来求解问题嘛，很明显，这是不行的，因为$l_2$范数会破坏数据的稀疏性，所以求解的出来的那个解对于我们来说是没有用的，简单点说就是$l_2$范数的解空间是与球相切的，它的解在坐标轴上是非稀疏的（有点粗略，不明白的可以去各大wiki上学学范数）。</p><h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><h3 id="线性规划问题"><a href="#线性规划问题" class="headerlink" title="线性规划问题"></a>线性规划问题</h3><p>​    一个比较基础方法就是将问题转化成线性规划问题。</p><p>​    首先我们再明确一下我们的问题：</p><script type="math/tex; mode=display">min||x||_1 \quad s.t.\quad Ax=b</script><p>​    那么我们首先将$x$定义成两个非负变量的差：</p><script type="math/tex; mode=display">x=u-v,u,v\geq 0</script><p>​    那么我们可以发现在这个顶一下$x$中的元素可以是正也可以是负，那么我们的原等式可以重写为：</p><script type="math/tex; mode=display">[A,-A][u,v]^T=b</script><p>​    也就是：</p><script type="math/tex; mode=display">||x||_1=\sum _{i=1}^n|x_i|=\sum _{i=1}^n|u_i-v_i|</script><p>​    所以我们就需要求解下列问题</p><script type="math/tex; mode=display">\begin{cases}min\sum (u+v)\\u-v=x\\u,v\geq0\end{cases}</script><p>​    我们再定义$y=[u,v]^T\in R^{2n}$,那么我们就可以得到一个较为标准的线性方程：</p><script type="math/tex; mode=display">\begin{cases}min\quad ey\\[A,-A]y=b\\y\geq 0\end{cases}</script><p>​    其中e为1行2n列元素全为 1 的向量。</p><p>具体代码如下：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[ alpha ]</span> = <span class="title">bpalg</span><span class="params">( Phi,b )</span></span></span><br><span class="line"><span class="comment">% 使用BP思想计算稀疏系数</span></span><br><span class="line"><span class="comment">%phi就是上文中对应的A，alpha就是我们还原的压缩系数，其他定义一致</span></span><br><span class="line">    n = <span class="built_in">size</span>(Phi, <span class="number">2</span>);<span class="comment">%取phi的列数</span></span><br><span class="line">    e = <span class="built_in">ones</span>(<span class="number">1</span>,<span class="number">2</span> * n);</span><br><span class="line">    A = [Phi, -Phi];</span><br><span class="line">    lb = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">2</span> * n);</span><br><span class="line">    y = linprog(<span class="number">0.5</span>*e,[],[],A,b,lb,[])</span><br><span class="line">    alpha = y(<span class="number">1</span> : n) - y(n + <span class="number">1</span> : <span class="number">2</span> * n);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h2 id="利用非光滑凸优化问题的最优性理论-还没搞懂"><a href="#利用非光滑凸优化问题的最优性理论-还没搞懂" class="headerlink" title="利用非光滑凸优化问题的最优性理论(还没搞懂)"></a>利用非光滑凸优化问题的最优性理论(还没搞懂)</h2><p>​    对于等式约束，我们引入Lagrangian乘子$v\in R^m$,则Lagrangian函数为：</p><script type="math/tex; mode=display">L(x,v)=||x||_1+V^T(Ax-b)</script><p>参考文献：</p><p><a href="https://blog.csdn.net/weixin_46584887/article/details/123958774">稀疏优化L1范数最小化问题求解之基追踪准则(Basis Pursuit)——原理及其Python实现-CSDN博客</a></p><p><a href="https://zhuanlan.zhihu.com/p/390844225">Basis Pursuit, LASSO, TV - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/theonegis/article/details/78268737">基追踪及其实现_atomic decomposition by basis pursuit-CSDN博客</a></p><p>未完待续……</p>]]></content>
      
      
      
        <tags>
            
            <tag> 压缩感知 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>压缩感知基本概念</title>
      <link href="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
      <url>/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="奈奎斯特采样定理"><a href="#奈奎斯特采样定理" class="headerlink" title="奈奎斯特采样定理"></a>奈奎斯特采样定理</h1><p>​    想要让采样之后的数字信号完整保留信号中的信息，采样频率<strong>必须大于信号中最高频率的两倍！</strong></p><h2 id="为什么？"><a href="#为什么？" class="headerlink" title="为什么？"></a>为什么？</h2><p>​    从下面这张图中可以看到，如果采样率比信号频率的两倍要低，那么在频域中信号会<strong>混叠</strong>，这个信号就和时域信号中的信号不一样了。</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/a.png" class="" title="a"><p>​    <strong>举个例子</strong>，对于一个$sin(2\pi x)$的信号，他的周期是$1$,频率就是$1$</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/Figure_1.png" class="" title="Figure_1"><p>​    我们可以看到在1Hz的采样频率的情况下，图像已经和我们的原函数相差很大了，当2Hz的采样频率下才有点像了，当10Hz就已经十分甚至九分地像了。<strong>当然这是对于一个比较标准的三角函数来进行的模拟，感兴趣的读者可以试着自己做一下多个不同周期的正弦波叠加的信号在不同采样频率下的结果。</strong></p><h1 id="概念的提出"><a href="#概念的提出" class="headerlink" title="概念的提出"></a>概念的提出</h1><p>​    所以说我们可以<strong>粗略地</strong>得出一个结论，采样的频率越高，<strong>还原出来的原始信号就越符合原始信号！</strong></p><p>​    但是，问题来了，我们一定要在那么高的采样率下进行数据收集嘛？很显然，这是一种资源的浪费，所以，在2004年，三位大牛证明出，如果信号是稀疏的，那么它就可以由远低于上述采样定理要求的采样点重构恢复，并于2007年正式提出了<strong>“压缩感知”</strong></p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/daniu.png" class="" title="就是这三位 daniu"><h1 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h1><p>​    在奈奎斯特采样理论中，我们使用的是<strong>采样频率</strong>，这就意味着我们进行的是等间距采样，而这样就会使得我们获得的信号在频域中以$\frac{1}{\tau}$为周期延拓，这就导致了<strong>混叠</strong>。</p><p>​    那我们应该如何规避掉这个问题呢，首先根据上文我们可以知道，混叠的根本原因就是<strong>等间距采样</strong>，所以，只有改变采样方式，我们才可以打破这个限制，于是乎，我们就需要进行<strong>随机采样</strong>。这样说可能有点含糊，给你们看两张图。</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/135f29c4eda0d7012849a939a19f02d3_720w.png" class="" title="135f29c4eda0d7012849a939a19f02d3_720w"><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/c8cd106fcb6ddd76bbb594a325083416_720w.png" class="" title="c8cd106fcb6ddd76bbb594a325083416_720w"><p>​    第一张图中采用下方的<strong>等间距采样</strong>，我们可以发现，由于频域信号周期延拓，我们的信号产生了混叠吗，而且是有规律有周期的混叠，那么我们从中恢复出原始信号的可能性是0。但是我们看第二张图，它是采用上方的上<strong>随机亚采样</strong>，虽然还是存在信号延拓，但是他不是有规律的，有周期的，它产生了大量<strong>不相关</strong>（<strong>注意，这个在后面很有用</strong>）的干扰值，但是呢，在图c中，我们原始信号的那几个峰值依稀可见，只是一定程度地被干扰值覆盖了，所以，根据这个特性，我们可以来重构我们原始信号了。</p><h1 id="怎么还原（形象化地表示）"><a href="#怎么还原（形象化地表示）" class="headerlink" title="怎么还原（形象化地表示）"></a>怎么还原（形象化地表示）</h1><p><strong>经典算法：匹配追踪</strong></p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/a4482144b2b18b754df69527ad0fbc0f_720w.png" class="" title="a4482144b2b18b754df69527ad0fbc0f_720w"><p>​    由于原信号的频域中非零值在采样过后的频域中任然保留了较大的值，那么其中部分节点，如<strong>图a</strong>中的的<strong>黄绿</strong>两个值，可以通过<strong>设置阈值</strong>来得到，具体流程就是图a-&gt;图b-&gt;图c。</p><p>​    有部分信号可能需要通过它单独的非零值和其自身产生的干扰值实现还原，也是通过<strong>设置阈值</strong>来实现，如图d。</p><h2 id="小结论"><a href="#小结论" class="headerlink" title="小结论"></a>小结论</h2><p>​    所以，到这里，我们就可以得到压缩感知理论的核心思想——以比奈奎斯特采样频率要求的采样密度更稀疏的密度对信号进行随机亚采样，由于频谱是均匀泄露的，而不是整体延拓的，因此可以通过特别的追踪方法将原信号恢复。</p><h1 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h1><h2 id="大前提"><a href="#大前提" class="headerlink" title="大前提"></a>大前提</h2><p>​    通过上面的例子，我们可以知道想要实现信号的恢复，在采样过程中必须保证两个大前提：</p><ol><li>信号在频域中<strong>稀疏</strong></li><li>采用随机亚采样，使得频率泄露均匀分布在频域中。</li></ol><h2 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h2><p>​    在某个域中只有少量非零值，则称之为稀疏，具体来说就是<strong>非零点远远小于信号总点数</strong>。</p><h2 id="一、如何稀疏？"><a href="#一、如何稀疏？" class="headerlink" title="一、如何稀疏？"></a>一、如何稀疏？</h2><p>​    那么问题来了，我们应该如果使得我们采集的信号是稀疏的呢？这就需要一点点数学知识了。</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/e92e1d7e46ed71ffd2e0ce7c10a82006_720w.png" class="" title="e92e1d7e46ed71ffd2e0ce7c10a82006_720w"><p>​    学过傅里叶变换的同学可能知道，任何函数都可以表示为n个正弦波和余弦波叠加得到（没学过的同学可以去百度，了解下基本概念就好了），这样我们就将<strong>时域信号</strong>转化为了频域信号，但是显然，傅里叶变化得到的大部分频域信号都是<strong>非稀疏</strong>的。</p><p>​    所以我们可以通过别的方法将时域信号转换到某个特定域，从而实现信号的稀疏化，常见的就是小波变换，离散余弦变换（DCT）这些。</p><h2 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a>数学表达</h2><p>说到这里，我们之前得到的结论都可以通过数学方式来得到。具体如下图所示。</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/caed3119833931f21fd97e8e09f0c098_720w.png" class="" title="caed3119833931f21fd97e8e09f0c098_720w"><p>​    其中$\Phi$为观测矩阵，就是我们压缩的数据，将<strong>高维数据压缩至低维数据</strong>的重点，通过矩阵乘法将n维的矩阵转换为m维的矩阵，其实就是一个投影的过程。</p><p>​    而之前提到我们需要对原始信号进行稀疏化，那怎么进行稀疏化呢？我们来看这张图中的$x$，它是通过$n\times n$的矩阵$\Psi$和$n\times 1$的矩阵s相乘的到，其中，x表示的是一个一维的原始信号，而$\Psi$则是我们上面说的对信号进行稀疏化操作的一个变换矩阵，而s则是我们得到的稀疏矩阵。</p><p>​    所以对原始信号的稀疏化我们就可以表示为$x=\Psi s$,相关数据的定义已在上一段给出。</p><p>​    所以总的流程就是$y=\Phi \Psi s$,其中$\Psi s$表示原始信号，s为原始信号的稀疏表达。</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/ebc9d3ab03de5d8156a2d82cc51fe04f_720w.png" class="" title="ebc9d3ab03de5d8156a2d82cc51fe04f_720w"><p>​    所以说，现在的问题就比较简单明了了，由于原公式有点长，我们令$\theta =\Phi \Psi$，在还原过程中，我们就只需要关注一个问题：在$y=\theta s$的情况下，y和$\theta$已知，如何求解s，这个放到文章后面来讲。</p><h2 id="二、不相关性"><a href="#二、不相关性" class="headerlink" title="二、不相关性"></a>二、不相关性</h2><p>​    前面我们提过我们需要使用<strong>随机</strong>亚采样才能实现我们需要的我们的目的，这就说明我们需要通过证明<strong>观测矩阵</strong>和稀疏表示基之间相关性很低才行，换句话来说就是$\Phi \Psi $不相关。具体证明可以看图了解一下。、</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/334eac83305e979e5a68f82a0045f97f_720w.png" class="" title="334eac83305e979e5a68f82a0045f97f_720w"><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/a187cdf32905b89c8f81b64d120b2d12_720w.png" class="" title="a187cdf32905b89c8f81b64d120b2d12_720w"><h1 id="观测矩阵-Phi-的选取"><a href="#观测矩阵-Phi-的选取" class="headerlink" title="观测矩阵$\Phi $的选取"></a>观测矩阵$\Phi $的选取</h1><p>​    一般来说，我们都是采用高斯矩阵，这个矩阵已经可以做到很好的效果了。</p><img src="/2023/11/24/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/8d694d3a31ecde541275b6ef6ff0cd0b_720w.webp" class="" title="8d694d3a31ecde541275b6ef6ff0cd0b_720w"><p>当然，如果你有别的办法来构建观测矩阵的话，<strong>请联系我带我上一区TOP，谢谢！</strong></p><h1 id="那么问题来了，有啥用？"><a href="#那么问题来了，有啥用？" class="headerlink" title="那么问题来了，有啥用？"></a>那么问题来了，有啥用？</h1><p>​    举个简单的例子，核磁共振的时候，采样时很慢的，你也不希望一直暴露在辐射里面吧，那么我们就可以通过压缩感知的方法来快速出图，还有比如在大规模无线网络中的应用，或者说在联邦学习等之中，都可以使用压缩感知进行优化。</p><p>参考文献：</p><p><a href="https://zhuanlan.zhihu.com/p/22445302">形象易懂讲解算法II——压缩感知 - 知乎 (zhihu.com)</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> 压缩感知 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
